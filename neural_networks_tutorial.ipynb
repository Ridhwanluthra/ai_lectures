{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Neural Networks In Pytorch\n",
    "==============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Neural networks can be constructed using the ``torch.nn`` package.\n",
    "\n",
    "An ``nn.Module`` contains layers, and a method ``forward(input)`` that takes the input tensor and output's the result of the neural network(or any other module)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pytorch provides a very powerful ``autograd`` package which is essential for calculating the derivates of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can divide the typical procedure for training a neural net into 5 steps:\n",
    "\n",
    "1. Define the neural network that has some learnable parameters (or\n",
    "  weights)\n",
    "3. Process input through the network aka _______\n",
    "4. Compute the loss (how far is the output from being correct) eg. _______\n",
    "5. Move gradients back into the network’s parameters Using _______\n",
    "6. Update the weights of the network, typically using a simple update rule:\n",
    "  ``weight = weight - learning_rate * gradient``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## First things first, time for some imports!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Define the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are 2 ways to define a simple neural network:\n",
    "1. Using built-in ``nn.Sequential``\n",
    "2. Creating our own custom class inheriting from ``nn.Module``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "num_inputs, num_hidden, num_outputs = 1000, 100, 10\n",
    "\n",
    "sequential_model = nn.Sequential(\n",
    "    nn.Linear(num_inputs, num_hidden),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(num_hidden, num_outputs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(num_inputs, num_hidden)\n",
    "        self.linear2 = torch.nn.Linear(num_hidden, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "custom_model = TwoLayerNet(num_inputs, num_hidden, num_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You just have to define the ``forward`` function, and the ``backward``\n",
    "function (where gradients are computed) is automatically defined for you\n",
    "using ``autograd`` (If you wanted to define custom backward function just overload the backword function in ``nn.Module``).\n",
    "You can use any of the Tensor operations in the ``forward`` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Let's print the networks and see what do we get..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "TwoLayerNet(\n",
      "  (linear1): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (linear2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(sequential_model)\n",
    "print(custom_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Wait What!!!! Are they different??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Let's check the number of trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential Model: 101110\n",
      "Custom Model: 101110\n"
     ]
    }
   ],
   "source": [
    "print(\"Sequential Model: {}\".format(sum(p.numel() for p in sequential_model.parameters() if p.requires_grad)))\n",
    "print(\"Custom Model: {}\".format(sum(p.numel() for p in custom_model.parameters() if p.requires_grad)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1746,  0.1649,  0.5953, -0.2792, -0.1438, -0.1933,  0.0381,  0.2224,\n",
      "          0.1697,  0.1465]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1000)\n",
    "out = custom_model(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "``torch.nn`` only supports mini-batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Before proceeding further, let's recap all the classes you’ve seen so far.\n",
    "\n",
    "**Recap:**\n",
    "  -  ``torch.Tensor`` - A *multi-dimensional array* with support for autograd\n",
    "     operations like ``backward()``. Also *holds the gradient* w.r.t. the\n",
    "     tensor.\n",
    "  -  ``nn.Module`` - Neural network module. *Convenient way of\n",
    "     encapsulating parameters*, with helpers for moving them to GPU,\n",
    "     exporting, loading, etc.\n",
    "  -  ``nn.Parameter`` - A kind of Tensor, that is *automatically\n",
    "     registered as a parameter when assigned as an attribute to a*\n",
    "     ``Module``.\n",
    "  -  ``autograd.Function`` - Implements *forward and backward definitions\n",
    "     of an autograd operation*. Every ``Tensor`` operation creates at\n",
    "     least a single ``Function`` node that connects to functions that\n",
    "     created a ``Tensor`` and *encodes its history*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Compute the Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A loss function takes the (output, target) pair of inputs, and computes a\n",
    "value that estimates how far away the output is from the target.\n",
    "\n",
    "There are several different\n",
    "[loss functions](https://pytorch.org/docs/nn.html#loss-functions>) under the\n",
    "nn package .\n",
    "A simple loss is: ``nn.MSELoss`` which computes the mean-squared error\n",
    "between the input and the target.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6360, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "target = torch.randn(1, 10)  # a dummy target, for example\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(out, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To backpropagate the error all we have to do is to ``loss.backward()``.\n",
    "You need to clear the existing gradients though, else gradients will be\n",
    "accumulated to existing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.bias.grad before backward\n",
      "None\n",
      "linear1.bias.grad after backward\n",
      "tensor([ 0.0000e+00,  8.8082e-02,  0.0000e+00,  4.9246e-02,  0.0000e+00,\n",
      "         0.0000e+00,  7.5798e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -5.1210e-03, -6.6060e-03, -3.7342e-02, -6.1204e-03,  7.1041e-02,\n",
      "         0.0000e+00,  0.0000e+00,  2.9379e-02,  4.1755e-02, -2.5439e-02,\n",
      "         0.0000e+00,  1.8737e-02,  9.4239e-03,  1.3639e-02, -3.6942e-02,\n",
      "         0.0000e+00, -1.0299e-03,  0.0000e+00,  0.0000e+00,  2.6386e-02,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  7.8013e-03, -1.3217e-02,\n",
      "        -1.7844e-02,  0.0000e+00,  0.0000e+00,  3.8579e-02,  8.7822e-03,\n",
      "        -2.1021e-05,  2.9333e-02,  0.0000e+00,  1.7800e-02, -5.9021e-03,\n",
      "         0.0000e+00,  7.0990e-03,  0.0000e+00,  1.6522e-02,  5.4553e-02,\n",
      "        -1.2442e-03,  0.0000e+00, -1.6706e-02,  0.0000e+00,  1.7266e-03,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.1967e-02,\n",
      "        -1.2956e-02, -7.4938e-03,  5.4445e-02,  0.0000e+00,  2.6752e-02,\n",
      "         3.8359e-02, -1.4794e-02,  0.0000e+00,  0.0000e+00,  7.8659e-02,\n",
      "         2.2141e-02, -1.0784e-01, -3.6253e-02,  0.0000e+00,  0.0000e+00,\n",
      "        -4.4291e-02,  0.0000e+00, -3.2187e-02,  2.3267e-02,  3.3654e-02,\n",
      "        -2.6265e-02,  0.0000e+00,  1.0745e-02,  2.7158e-02,  6.3135e-02,\n",
      "        -1.3024e-02,  0.0000e+00,  0.0000e+00, -1.6001e-02,  0.0000e+00,\n",
      "         3.0887e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         5.7829e-02,  2.8839e-03,  3.9362e-03,  0.0000e+00,  2.1077e-02])\n"
     ]
    }
   ],
   "source": [
    "custom_model.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('linear1.bias.grad before backward')\n",
    "print(custom_model.linear1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('linear1.bias.grad after backward')\n",
    "print(custom_model.linear1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "loss function are very important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The simplest update rule used in practice is the Stochastic Gradient\n",
    "Descent (SGD):\n",
    "\n",
    "     ``weight = weight - learning_rate * gradient``\n",
    "\n",
    "We can implement this using simple python code:\n",
    "``\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n",
    "``\n",
    "\n",
    "However, as you use neural networks, you want to use various different\n",
    "update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.\n",
    "To enable this, They built a small package: ``torch.optim`` that\n",
    "implements all these methods. Using it is very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# create your optimizer\n",
    "optimizer = optim.SGD(custom_model.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = custom_model(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why do we have to zero the gradient buffers manually?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
